# backend/Dockerfile.worker
# Cloud worker image with SAM3 model baked in
#
# Build (with BuildKit for cache mounts):
#   DOCKER_BUILDKIT=1 docker build -f Dockerfile.worker -t basketball-analyzer-worker .
#
# Run:
#   docker run --gpus all -e R2_ACCOUNT_ID=xxx -e R2_ACCESS_KEY_ID=xxx \
#     -e R2_SECRET_ACCESS_KEY=xxx -e R2_BUCKET_NAME=xxx \
#     basketball-analyzer-worker

# ============================================================================
# Stage 1: Export requirements from poetry (small, fast)
# ============================================================================
FROM python:3.11-slim AS requirements

WORKDIR /build
RUN pip install poetry

COPY pyproject.toml poetry.lock* ./
RUN poetry export --only main,ml,video --without-hashes -f requirements.txt -o requirements.txt

# ============================================================================
# Stage 2: Runtime image
# ============================================================================
FROM pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime

WORKDIR /app

# Install system dependencies (rarely changes - cached)
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Copy SAM3 model EARLY - it's huge (~6GB) but rarely changes
# This layer gets cached and reused across code changes
COPY models/sam3 /models/sam3

# Copy exported requirements and install with pip (faster than poetry)
# Using BuildKit cache mount for pip cache across builds
COPY --from=requirements /build/requirements.txt ./
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --no-deps -r requirements.txt

# Copy application code LAST - changes most frequently
COPY app/ ./app/
COPY worker/ ./worker/

# Environment
ENV PYTHONPATH=/app
ENV CLOUD_MODEL_PATH=/models/sam3

CMD ["python", "-m", "worker", "--cloud"]
