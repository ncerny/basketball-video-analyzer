{"id":"bbva-03i","title":"Implement video metadata extraction service","description":"Extract duration, fps, resolution, recorded_at timestamp using FFmpeg/OpenCV","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.776919-06:00","updated_at":"2025-12-07T16:58:10.88816-06:00","closed_at":"2025-12-07T16:58:10.88816-06:00","dependencies":[{"issue_id":"bbva-03i","depends_on_id":"bbva-fft","type":"blocks","created_at":"2025-12-06T19:26:38.70882-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-0bw","title":"Implement jersey number OCR service","description":"Extract jersey numbers from player detections using OCR to enable automatic player identification. This service will process bounding boxes from YOLO detections and extract visible jersey numbers.","design":"## Current State\n- YOLO detections identify player bounding boxes with high accuracy\n- ByteTrack provides persistent tracking IDs across frames\n- Court detection filters out audience members\n- No jersey number extraction capability yet\n\n## Approach\nUse EasyOCR or Tesseract for OCR with preprocessing:\n1. Extract cropped player regions from detections\n2. Preprocess images (resize, enhance contrast, denoise)\n3. Run OCR to extract numbers (focus on numeric characters only)\n4. Validate results (basketball jerseys typically 0-99)\n5. Store OCR results in new table with confidence scores\n6. Aggregate results across multiple frames for same tracking_id\n\n## Integration Points\n- Input: PlayerDetection records with bbox coordinates\n- Process: Extract frame region, preprocess, OCR\n- Output: JerseyNumber table (detection_id, tracking_id, number, confidence)\n- Pattern: Similar to court_detector.py - standalone service class\n\n## Performance Considerations\n- Run OCR in thread pool (asyncio.to_thread) to avoid blocking\n- Only OCR every Nth frame per tracking_id to reduce compute\n- Cache results by tracking_id to avoid redundant processing\n\n## Files to Create\n- backend/app/ml/jersey_ocr.py - OCR service class\n- backend/app/models/jersey_number.py - Database model\n- backend/alembic/versions/*_add_jersey_numbers.py - Migration","acceptance_criteria":"## Automated Tests\n- Unit test: OCR extracts correct numbers from sample cropped images\n- Unit test: Image preprocessing improves OCR accuracy\n- Unit test: Validation rejects invalid jersey numbers (\u003e99, negative)\n- Unit test: Aggregation combines results from multiple frames\n\n## Manual Testing\n- Process video with visible jersey numbers\n- Verify OCR correctly identifies at least 70% of visible numbers\n- Check that results improve when aggregating across frames\n- Confirm no blocking of API during OCR processing","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-19T08:02:06.333405-06:00","updated_at":"2025-12-20T14:31:04.941845-06:00","closed_at":"2025-12-20T14:31:04.941845-06:00"}
{"id":"bbva-0co","title":"Integrate OCR with detection pipeline","description":"Run OCR on sampled frames during detection","design":"Modify detection_pipeline.py: Apply legibility filter before OCR. Run SmolVLM2 on qualifying crops. Run in asyncio.to_thread. Store results to jersey_number table. Sample every Nth frame per track to reduce compute.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T00:04:26.732243-06:00","updated_at":"2025-12-20T12:06:13.68882-06:00","closed_at":"2025-12-20T12:06:13.68882-06:00"}
{"id":"bbva-0dx","title":"Integrate SAM2 tracking backend with Supervision migration","description":"Add SAM2 as alternative tracking backend to address ByteTrack's IOU-only matching failures on fast basketball motion (268 tracks for ~10 players).","design":"## Architecture\n- Add SAM2 as third tracking backend alongside ByteTrack and Norfair\n- Migrate ByteTrack to Supervision library for cleaner API\n- SAM2 uses appearance memory for re-identification through occlusions\n\n## Key Files\n- backend/app/ml/sam2_tracker.py (new)\n- backend/app/ml/byte_tracker.py (rewrite with Supervision)\n- backend/app/config.py (add sam2 options)\n- backend/app/services/batch_processor.py (pass frame to tracker)\n\n## Success Criteria\n- Reduce track count from 268 to \u003c30 for 10-player video\n- \u003c100ms per frame on MPS","notes":"Implementation complete. Files created/modified:\n- pyproject.toml: Added sam2 ^1.0.0, updated torch\u003e=2.3.1\n- config.py: Added sam2 tracking_backend option and config settings\n- sam2_tracker.py: New SAM2VideoTracker class with memory-based tracking\n- download_sam2_model.py: Model download script\n- batch_processor.py: Integrated SAM2 tracker with frame passing\n- detection_pipeline.py: Integrated SAM2 tracker with frame passing\n\nReady for testing. Need to: pip install sam2, download model, set tracking_backend=sam2","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-24T00:16:25.997628-06:00","updated_at":"2025-12-24T00:55:42.547292-06:00","closed_at":"2025-12-24T00:55:42.547292-06:00","close_reason":"SAM2 tracking backend integrated successfully. Uses SAM2 image predictor for frame-by-frame mask-based tracking. Testing showed 13 unique tracks for 10 simulated players (vs 268 with ByteTrack). Key changes: sam2_tracker.py using SAM2ImagePredictor, config.py adds sam2 backend option, batch_processor.py and detection_pipeline.py pass frames to SAM2 tracker."}
{"id":"bbva-0t9","title":"Implement Game Detail page","description":"Show game info, videos, roster, integration with player","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.548482-06:00","updated_at":"2025-12-08T14:15:12.441191-06:00","closed_at":"2025-12-08T14:15:12.441191-06:00","dependencies":[{"issue_id":"bbva-0t9","depends_on_id":"bbva-t11","type":"blocks","created_at":"2025-12-06T19:26:59.967704-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-0t9","depends_on_id":"bbva-sl9","type":"blocks","created_at":"2025-12-06T19:27:00.015557-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-0t9","depends_on_id":"bbva-f83","type":"blocks","created_at":"2025-12-06T19:27:00.060211-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-0ty","title":"Implement Video Analysis page","description":"Main analysis interface with timeline player and annotations","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.592855-06:00","updated_at":"2025-12-08T23:24:44.619612-06:00","closed_at":"2025-12-08T23:24:44.619612-06:00","dependencies":[{"issue_id":"bbva-0ty","depends_on_id":"bbva-6c1","type":"blocks","created_at":"2025-12-06T19:27:00.105778-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-0ty","depends_on_id":"bbva-4m7","type":"blocks","created_at":"2025-12-06T19:27:00.150134-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-11r","title":"Enhance SAM2 tracker with embedding memory and cross-video re-identification","description":"Improve SAM2 tracking with feature embeddings for robust player re-identification across occlusions, video segments, and games.","design":"## Problem\n\nCurrent SAM2 tracker has limitations:\n- Only compares against last mask (no history)\n- Mask IOU fails when player pose changes significantly\n- No persistence across video segments or games\n- No automatic model download\n\n## Solution: Two-Tier Embedding Architecture\n\n### Tier 1: In-Memory Track State (during video processing)\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   SAM2VideoTracker                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Frame Processing                                    â”‚\nâ”‚  - SAM2 Image Predictor generates masks              â”‚\nâ”‚  - SAM2 Encoder extracts embeddings per detection    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  In-Memory Track State                               â”‚\nâ”‚  - Track ID â†’ embedding + color histogram            â”‚\nâ”‚  - Kept for entire video duration                    â”‚\nâ”‚  - Matching: embedding similarity (primary)          â”‚\nâ”‚              color histogram (tiebreaker)            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Tier 2: Database Persistence (cross-video/game)\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              player_embeddings table                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  - Store best embedding per track at video end       â”‚\nâ”‚  - Load at video start for cross-video matching      â”‚\nâ”‚  - Link to player records for cross-game identity    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Matching Flow Per Frame\n\n1. Generate masks + embeddings for new detections\n2. Compare against all active tracks (embedding cosine similarity)\n3. If close scores, use color histogram as tiebreaker\n4. If no match above threshold, check database for known players\n5. If still no match, create new track\n\n## Implementation\n\n### New Files\n- `backend/app/ml/embedding_extractor.py` - Extract embeddings from SAM2 encoder\n- `backend/app/models/player_embedding.py` - SQLAlchemy model\n- `backend/scripts/download_sam2_model.py` - Auto-download models\n\n### Modified Files\n- `backend/app/ml/sam2_tracker.py` - Add embedding memory, color matching\n- `backend/app/config.py` - New settings for embedding matching\n- `backend/app/services/batch_processor.py` - Persist embeddings at video end\n\n### New Settings\n```python\nsam2_embedding_similarity_threshold: float = 0.7\nsam2_color_tiebreaker_threshold: float = 0.1  # Score diff to trigger tiebreaker\nsam2_reidentification_enabled: bool = True\n```\n\n## Future Enhancements (documented, not implemented)\n\n### Video Predictor Mode\nSAM2's Video Predictor offers built-in memory and mask propagation but requires:\n- Frames saved to disk as JPEGs\n- Batch processing (not streaming)\n- Could be added as post-processing refinement step\n\n### Keyframe Masks\nStore high-quality masks at key moments for additional matching signal:\n- Useful when embeddings are similar between players\n- Higher memory cost (~100KB-1MB per mask)\n- Could combine with embeddings for hybrid matching","acceptance_criteria":"## Automated Tests\n- Unit test: Embedding extraction produces consistent vectors\n- Unit test: Cosine similarity matching works correctly\n- Unit test: Color tiebreaker activates when scores are close\n- Unit test: Auto-download fetches missing models\n- Integration test: Track maintains ID through brief occlusion\n\n## Manual Testing\n- Run detection on test video with player occlusions\n- Verify tracks persist through occlusions\n- Test cross-video matching (process two segments, verify same IDs)\n- Confirm model auto-downloads on first use\n\n## Performance Goals\n- Reduce ID switches by 50% compared to mask-only matching\n- Cross-video re-identification accuracy \u003e80%\n- Embedding extraction adds \u003c10ms per detection","notes":"Phase 1-3 complete with bug fixes:\n\n**Completed:**\n- OCR disabled in config and properly propagated to job metadata\n- Auto-download SAM2 checkpoints (using SAM2 072824, not SAM2.1 for library compatibility)\n- Embedding extraction from SAM2 encoder with ROI pooling\n- Cosine similarity matching with EMA blending\n- Color histogram tiebreaker when scores within threshold\n- Tracks kept for entire video (lost_track_frames=0)\n- Debug logging for troubleshooting\n\n**Config changes:**\n- sam2_embedding_similarity_threshold: 0.5\n- sam2_color_tiebreaker_threshold: 0.15\n- sam2_lost_track_frames: 0 (keep all tracks)\n\n**Bug fixes:**\n- Fixed SAM2.1 vs SAM2 checkpoint compatibility\n- Fixed OCR not respecting enable_jersey_ocr setting (multiple places)\n- Synced SAM2TrackerConfig defaults with global settings\n\n**Pending:** Phase 4-5 (database persistence, cross-video matching)","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-12-24T12:00:58.679819-06:00","updated_at":"2025-12-26T12:13:44.896019-06:00"}
{"id":"bbva-1fw","title":"End-to-end testing with real video","description":"Test with real basketball footage, handle edge cases","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.727277-06:00","updated_at":"2025-12-15T16:15:42.978877-06:00","closed_at":"2025-12-15T16:15:42.978877-06:00","dependencies":[{"issue_id":"bbva-1fw","depends_on_id":"bbva-0ty","type":"blocks","created_at":"2025-12-06T19:27:00.522938-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-2uy","title":"Implement batch-based pipeline with checkpointing","design":"Refactor detection and OCR into batch-based processors with checkpoint/resume capability. See docs/implementation-plan.md for full design.","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-12-20T15:32:56.401813-06:00","updated_at":"2025-12-20T15:53:31.909434-06:00"}
{"id":"bbva-2uz","title":"Implement video frame extraction service","description":"Service to extract frames from video files using OpenCV. Supports sampling (every N frames), time-based extraction, and batch extraction. Core component for the player detection pipeline.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-18T04:00:47.495089-06:00","updated_at":"2025-12-18T04:04:30.417604-06:00","closed_at":"2025-12-18T04:04:30.417604-06:00"}
{"id":"bbva-37q","title":"Update CLAUDE.md with proper git workflow and beads commands","description":"Add critical warning about never pushing to main, document beads workflow from bd prime, update git workflow steps to always use PRs","status":"closed","priority":0,"issue_type":"chore","created_at":"2025-12-08T14:02:57.842695-06:00","updated_at":"2025-12-15T16:14:07.139386-06:00","closed_at":"2025-12-15T16:14:07.139386-06:00"}
{"id":"bbva-3qw","title":"Clarify beads workflow: exclude .beads/ from feature branch commits","description":"Update CLAUDE.md to clarify that .beads/ changes should NOT be included in feature branch commits. bd sync handles beads changes separately by pushing directly to main.","status":"closed","priority":0,"issue_type":"chore","created_at":"2025-12-08T17:23:45.704167-06:00","updated_at":"2025-12-08T23:20:04.56882-06:00","closed_at":"2025-12-08T23:20:04.56882-06:00"}
{"id":"bbva-3yt","title":"Implement jersey number OCR service","description":"Extract jersey numbers from player detections using OCR to enable automatic player identification. This service will process bounding boxes from YOLO detections and extract visible jersey numbers.","design":"## Current State\n- YOLO detections identify player bounding boxes with high accuracy\n- ByteTrack provides persistent tracking IDs across frames\n- Court detection filters out audience members\n- No jersey number extraction capability yet\n\n## Approach\nUse EasyOCR or Tesseract for OCR with preprocessing:\n1. Extract cropped player regions from detections\n2. Preprocess images (resize, enhance contrast, denoise)\n3. Run OCR to extract numbers (focus on numeric characters only)\n4. Validate results (basketball jerseys typically 0-99)\n5. Store OCR results in new table with confidence scores\n6. Aggregate results across multiple frames for same tracking_id\n\n## Integration Points\n- Input: PlayerDetection records with bbox coordinates\n- Process: Extract frame region, preprocess, OCR\n- Output: JerseyNumber table (detection_id, tracking_id, number, confidence)\n- Pattern: Similar to court_detector.py - standalone service class\n\n## Performance Considerations\n- Run OCR in thread pool (asyncio.to_thread) to avoid blocking\n- Only OCR every Nth frame per tracking_id to reduce compute\n- Cache results by tracking_id to avoid redundant processing","acceptance_criteria":"## Automated Tests\n- Unit test: OCR extracts correct numbers from sample cropped images\n- Unit test: Image preprocessing improves OCR accuracy\n- Unit test: Validation rejects invalid jersey numbers (\u003e99, negative)\n- Unit test: Aggregation combines results from multiple frames\n\n## Manual Testing\n- Process video with visible jersey numbers\n- Verify OCR correctly identifies at least 70% of visible numbers\n- Check that results improve when aggregating across frames\n- Confirm no blocking of API during OCR processing","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-19T08:01:56.871787-06:00","updated_at":"2025-12-19T08:08:15.68924-06:00","closed_at":"2025-12-19T08:08:15.68924-06:00"}
{"id":"bbva-43e","title":"Implement player tracking service with ByteTrack","description":"Implement ByteTrack algorithm for tracking players across video frames. Track player IDs consistently, handle occlusions, and persist tracking data to database. Dependencies: detection pipeline (bbva-ck1). Part of Phase 2 Computer Vision (bbva-g2m).","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T13:29:57.58044-06:00","updated_at":"2025-12-18T17:42:07.346227-06:00","closed_at":"2025-12-18T17:42:07.346227-06:00"}
{"id":"bbva-45w","title":"Create seed data for testing","description":"Create sample games, players, and test data for development","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:12.685045-06:00","updated_at":"2025-12-08T08:07:42.272963-06:00","closed_at":"2025-12-08T08:07:42.272963-06:00","dependencies":[{"issue_id":"bbva-45w","depends_on_id":"bbva-fft","type":"blocks","created_at":"2025-12-06T19:26:38.616-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-4f5","title":"Create SmolVLM2 OCR service","description":"Implement SmolVLM2 wrapper for jersey number reading","design":"Create backend/app/ml/jersey_ocr.py. Install transformers\u003e=4.49.0. Handle model loading/caching. Prompt engineering for number extraction. Return parsed number with confidence. Unit tests with sample crops.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T00:04:02.086262-06:00","updated_at":"2025-12-20T00:14:17.332578-06:00","closed_at":"2025-12-20T00:14:17.332578-06:00"}
{"id":"bbva-4f8","title":"SAM3 memory leak investigation - additional pruning needed","description":"Current `_prune_old_frames()` only prunes 3 structures but SAM3 accumulates at least 15. Seeing ~2GB net leak per pruning cycle (grows 10GB, drops 8GB).\n\n## Root Cause\nThe pruning misses several high-memory structures:\n\n### HIGH PRIORITY (likely causing the 2GB leak):\n\n1. **Vision features cache** (`inference_session._cache`)\n   - Stores computed image embeddings per frame\n   - ~50-100MB per frame\n   - NOT being pruned\n\n2. **Conditioning frame outputs** (`cond_frame_outputs`)\n   - Full outputs from prompted frames\n   - ~100-200MB per conditioning frame per object\n   - We only prune `non_cond_frame_outputs`\n\n3. **Processed frames** (`processed_frames`)\n   - Actual pixel tensors\n   - ~5-10MB per frame\n\n4. **Mask/point inputs** (`mask_inputs_per_obj`, `point_inputs_per_obj`)\n   - Stored prompts per frame/object\n\n## Memory Footprint (500-frame window, 2 objects)\n| Structure | Per Frame | Pruned? | Est. 500 frames |\n|-----------|-----------|---------|-----------------|\n| vision_features cache | 50-100MB | âŒ | 25-50GB |\n| non_cond_frame_outputs | 50-100MB | âœ… | (pruned) |\n| cond_frame_outputs | 100-200MB | âŒ | Variable |\n| processed_frames | 5-10MB | âŒ | 2.5-5GB |\n\n## Official APIs to Consider\n- `reset_tracking_data()` - clears tracking but keeps vision cache\n- `reset_inference_session()` - full reset\n- `inference_state_device=\"cpu\"` - reduces VRAM ~80%\n\n## Sources\n- https://huggingface.co/docs/transformers/main/en/model_doc/sam3_video\n- https://github.com/facebookresearch/sam2/blob/main/sam2/modeling/sam2_base.py\n- https://discuss.huggingface.co/t/sam2-video-streaming-vram-usage-keeps-increasing-until-oom/168526","design":"## Fix Strategy\n\nUpdate `_prune_old_frames()` in `sam3_tracker.py` to also prune:\n\n```python\n# 1. Prune conditioning frame outputs\nif \"cond_frame_outputs\" in obj_outputs:\n    cond = obj_outputs[\"cond_frame_outputs\"]\n    for f in [f for f in cond.keys() if f \u003c cutoff_frame]:\n        del cond[f]\n\n# 2. Prune vision features cache\nif hasattr(inference_session, \"_cache\"):\n    cache = inference_session._cache\n    if hasattr(cache, \"vision_features\"):\n        for f in [f for f in cache.vision_features.keys() if f \u003c cutoff_frame]:\n            del cache.vision_features[f]\n\n# 3. Prune processed frames\nif hasattr(inference_session, \"processed_frames\"):\n    for f in [f for f in inference_session.processed_frames.keys() if f \u003c cutoff_frame]:\n        del inference_session.processed_frames[f]\n\n# 4. Prune mask/point inputs per object\n# (see full implementation in research notes)\n\n# 5. Force MPS cache clear after pruning\nif hasattr(torch, 'mps') and torch.mps.is_available():\n    torch.mps.empty_cache()\n```\n\n## Risk\nPruning `cond_frame_outputs` may impact tracking quality for objects that were initially prompted - need to test.","acceptance_criteria":"- [ ] Memory stays bounded (no net growth per cycle)\n- [ ] 8000+ frame video completes without OOM on 64GB system\n- [ ] Tracking ID stability maintained (no regression)","notes":"Implemented additional pruning in _prune_old_frames(). Now prunes:\n- cond_frame_outputs\n- vision_features cache  \n- processed_frames\n- mask_inputs_per_obj\n- point_inputs_per_obj\n- Added gc.collect() + torch.mps.empty_cache() after pruning\n\nNeeds testing to verify memory stays bounded and tracking IDs remain stable.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-21T06:50:32.887763-06:00","updated_at":"2026-01-21T19:38:33.011799-06:00","closed_at":"2026-01-21T19:38:33.011799-06:00","close_reason":"Implemented what's safely prunable on MPS. Key findings:\n\n## What We Tried\n1. âœ… Original pruning (non_cond_frame_outputs, tracker scores, frames_tracked) - works\n2. âŒ cond_frame_outputs - breaks tracking (\"maskmem_features cannot be empty\")\n3. âŒ vision_features cache - MPS dtype mismatch errors\n4. âŒ processed_frames - MPS dtype mismatch errors  \n5. âŒ mask_inputs_per_obj / point_inputs_per_obj - MPS dtype mismatch errors\n6. âŒ torch.mps.empty_cache() - MPS dtype mismatch errors\n\n## Conclusion\nSAM3's architecture fundamentally requires keeping most memory structures intact for quality tracking. The prunable structures (metadata only) don't significantly reduce memory.\n\n**Memory is unbounded by design** - SAM3 needs historical context for its temporal memory mechanism.\n\n## Path Forward\n- bbva-gl2: Cloud GPU processing (A100 80GB fits in VRAM)\n- bbva-o1u: Investigate SAM2Long (bounded memory, but may have SAM2's ID drift issues)"}
{"id":"bbva-4m7","title":"Implement annotation interface","description":"Timeline markers, annotation form, player selection, multi-video handling","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.368534-06:00","updated_at":"2025-12-07T22:35:58.007208-06:00","closed_at":"2025-12-07T22:35:58.007208-06:00","dependencies":[{"issue_id":"bbva-4m7","depends_on_id":"bbva-rx5","type":"blocks","created_at":"2025-12-06T19:26:59.602301-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-4xa","title":"Implement video sequencing UI","description":"Drag-and-drop video ordering, manual offset adjustment, gap visualization","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.322216-06:00","updated_at":"2025-12-15T16:14:44.151879-06:00","closed_at":"2025-12-15T16:14:44.151879-06:00","dependencies":[{"issue_id":"bbva-4xa","depends_on_id":"bbva-rx5","type":"blocks","created_at":"2025-12-06T19:26:59.556883-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-56k","title":"Implement Annotations CRUD API endpoints","description":"Create, edit, delete annotations with multi-video support","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.095262-06:00","updated_at":"2025-12-07T21:59:08.879738-06:00","closed_at":"2025-12-07T21:59:08.879738-06:00","dependencies":[{"issue_id":"bbva-56k","depends_on_id":"bbva-y0h","type":"blocks","created_at":"2025-12-06T19:26:39.251581-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-5gm","title":"Enhance processing status UI with detailed progress","description":"Improve job status display to show detailed pipeline stage progress (extraction, detection, tracking, OCR, matching) with frame counts and estimated time remaining.","design":"## Current State\n- GameDetail.tsx shows basic progress bars for detection jobs\n- Polling every 2 seconds for job status updates\n- Progress shows percentage and generic message\n- No breakdown by pipeline stage (extraction, detection, tracking, etc.)\n\n## Approach\nEnhanced status display with stage-specific progress:\n1. Backend: Update job progress messages to include stage info\n2. Parse progress message to extract stage and metrics\n3. Show multi-stage progress bar (like npm install output)\n4. Display metrics: frames processed, detections found, tracking IDs\n5. Show estimated time remaining based on current rate\n6. Add stage icons (extraction â–¶ï¸, detection ğŸ‘ï¸, tracking ğŸ¯, OCR ğŸ”¢)\n\n## Implementation Details\n- Extend JobResponse.progress to include stages array\n- Each stage: {name, current, total, status: 'pending'|'active'|'complete'}\n- Update detection_pipeline.py to report structured progress\n- Create ProcessingStatusCard.tsx component\n- Use existing job polling pattern from GameDetail.tsx\n\n## Files to Modify\n- backend/app/services/job_manager.py - Add stages to progress tracking\n- backend/app/services/detection_pipeline.py - Report stage-specific progress\n- frontend/src/components/ProcessingStatusCard.tsx - New component\n- frontend/src/pages/GameDetail.tsx - Use new status card","acceptance_criteria":"## Automated Tests\n- Component test: ProcessingStatusCard renders multiple stages\n- Component test: Active stage highlighted correctly\n- Unit test: Progress message parsing extracts stage info\n- API test: Job status includes stages array\n\n## Manual Testing\n- Start detection job and watch status updates\n- Verify each stage shows progress separately\n- Check estimated time remaining updates\n- Confirm all stages marked complete when job finishes","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T08:03:09.840835-06:00","updated_at":"2025-12-19T08:03:09.840835-06:00"}
{"id":"bbva-61o","title":"Separate detection worker from backend process","description":"SAM3 detection blocks the FastAPI backend's event loop, making API calls unresponsive during processing. MPS (Metal) operations are synchronous and can't be truly async, even with `await asyncio.sleep(0)` yields.\n\n## Current Architecture\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         FastAPI Backend             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ API Routesâ”‚  â”‚ Detection Job  â”‚  â”‚\nâ”‚  â”‚           â”‚  â”‚ (blocks here)  â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Problem\n- SAM3 uses MPS which must run synchronously\n- 7 min per 30 frames = long blocking periods\n- API calls timeout or queue up during detection\n- Single process can't serve API and run ML simultaneously\n\n## Proposed Architecture\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   FastAPI Backend    â”‚     â”‚   Detection Worker   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  API Routes    â”‚  â”‚     â”‚  â”‚  SAM3 Tracker  â”‚  â”‚\nâ”‚  â”‚  Job Managementâ”‚  â”‚     â”‚  â”‚  DB Writer     â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚          â”‚           â”‚     â”‚          â”‚           â”‚\nâ”‚          â–¼           â”‚     â”‚          â–¼           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚   Database     â”‚â—„â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â–ºâ”‚   Database     â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        (responsive)              (can block freely)\n```\n\n## Communication via Database\n- Backend creates job record with status='pending'\n- Worker polls for pending jobs\n- Worker updates status='processing', then 'completed'/'failed'\n- Backend queries job status for progress","design":"## Implementation Plan\n\n### 1. Create worker module (`backend/worker/`)\n```\nbackend/worker/\nâ”œâ”€â”€ __init__.py\nâ”œâ”€â”€ __main__.py      # Entry point: python -m worker\nâ”œâ”€â”€ job_processor.py # Main loop: poll, process, update\nâ””â”€â”€ config.py        # Worker-specific settings (poll interval, etc.)\n```\n\n### 2. Job processor loop\n```python\n# worker/job_processor.py\nasync def run_worker():\n    while True:\n        job = await poll_for_pending_job()\n        if job:\n            await process_job(job)  # SAM3 detection\n        else:\n            await asyncio.sleep(poll_interval)\n```\n\n### 3. Update job_manager for worker model\n- Remove in-process job execution\n- Backend only creates/queries jobs\n- Add job status polling endpoint if not exists\n\n### 4. Process management\nOptions (pick one):\n- **Simple**: Manual start via `python -m worker`\n- **Systemd/launchd**: OS-level service management\n- **Supervisor**: Process manager for both backend + worker\n- **Docker Compose**: Separate containers\n\n### 5. Graceful shutdown\n- Worker checks for shutdown signal between frames\n- Commits current progress before exit\n- Job can resume from last checkpoint\n\n## Files to Create/Modify\n| File | Action |\n|------|--------|\n| `backend/worker/__main__.py` | NEW: Worker entry point |\n| `backend/worker/job_processor.py` | NEW: Job polling and processing |\n| `backend/app/services/job_manager.py` | MODIFY: Remove in-process execution |\n| `backend/app/models/job.py` | MODIFY: Add worker_id, started_at fields |\n| `scripts/start_worker.sh` | NEW: Convenience script |\n\n## Configuration\n```python\n# Worker settings\nWORKER_POLL_INTERVAL: int = 5  # seconds\nWORKER_MAX_CONCURRENT_JOBS: int = 1  # SAM3 is memory-heavy\nWORKER_SHUTDOWN_TIMEOUT: int = 300  # wait for current frame\n```","acceptance_criteria":"- [ ] Backend API remains responsive during detection\n- [ ] Worker processes jobs from database queue\n- [ ] Job progress visible via API during processing\n- [ ] Graceful shutdown: worker completes current frame before exit\n- [ ] Worker can resume interrupted jobs from last checkpoint\n- [ ] Simple start command: `python -m worker`","notes":"## Implementation Complete (2026-01-21)\n\n### Created Files:\n- `backend/worker/__init__.py` - Module init\n- `backend/worker/__main__.py` - Entry point (`python -m worker`)\n- `backend/worker/config.py` - Worker config (poll interval, worker ID, etc.)\n- `backend/worker/job_processor.py` - Polling loop and job execution\n- `backend/app/models/processing_job.py` - DB-backed job model (ProcessingJob)\n- `backend/app/services/job_service.py` - Job creation/query functions\n- `backend/scripts/start_worker.sh` - Convenience script\n- `backend/alembic/versions/882ba92dc242_add_processing_jobs_table.py` - Migration\n\n### Modified Files:\n- `backend/app/config.py` - Added `use_external_worker` setting (default: True)\n- `backend/app/api/detection.py` - Support both DB-backed and in-memory jobs\n- `backend/app/models/__init__.py` - Export new models\n\n### Usage:\n1. Start API: `uvicorn app.main:app`\n2. Start worker: `python -m worker` (or `./scripts/start_worker.sh`)\n3. Submit job via API: `POST /api/videos/{id}/detect`\n4. Worker picks up job and processes it\n\n### Configuration:\n- `USE_EXTERNAL_WORKER=true` (default) - Jobs queue in DB for worker\n- `USE_EXTERNAL_WORKER=false` - Jobs run in API process (legacy)\n\n### Not Yet Tested:\n- Full end-to-end detection job processing\n- Progress reporting during long jobs\n- Graceful shutdown with checkpoint resume","status":"in_progress","priority":2,"issue_type":"feature","created_at":"2026-01-21T10:07:50.648241-06:00","updated_at":"2026-01-21T19:46:15.163161-06:00"}
{"id":"bbva-6c1","title":"Implement game timeline player UI component","description":"Video player with unified timeline, playback controls, frame-by-frame navigation","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.230334-06:00","updated_at":"2025-12-08T23:23:20.03398-06:00","closed_at":"2025-12-08T23:23:20.03398-06:00","dependencies":[{"issue_id":"bbva-6c1","depends_on_id":"bbva-wi7","type":"blocks","created_at":"2025-12-06T19:26:59.46755-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-6ou","title":"Improve detection accuracy with fine-tuning and post-processing","description":"Enhance player detection accuracy by fine-tuning YOLOv8 on basketball-specific data, implementing temporal smoothing for tracking, and adding post-processing filters to reduce false positives.","design":"## Current State\n- Using pre-trained YOLOv8-nano for person detection\n- ByteTrack tracking with IOU threshold 0.8\n- Court detection filters spatial false positives (bbva-l8b)\n- Confidence threshold: 0.5\n- No domain-specific fine-tuning yet\n\n## Approach\n\n### 1. Fine-Tuning YOLOv8 on Basketball Data\n- Collect/annotate basketball-specific training data:\n  * Youth basketball videos with player annotations\n  * Variety of angles, lighting conditions, court surfaces\n  * 500-1000 annotated frames minimum\n- Fine-tune YOLOv8-nano on basketball dataset:\n  * Focus on players in basketball uniforms\n  * Learn to distinguish players from referees, coaches, audience\n  * Improve detection at various distances and angles\n- Use transfer learning: freeze backbone, train detection head\n- Validate on held-out test set\n\n### 2. Temporal Smoothing for Tracking\n- Implement Kalman filter for motion prediction\n- Smooth bounding box jitter across frames\n- Fill short gaps when player temporarily occluded\n- Use velocity estimation to predict next position\n- Helps ByteTrack maintain tracks through occlusions\n\n### 3. Post-Processing Filters\n- Size filtering: Remove detections too small/large for players\n- Aspect ratio filtering: Players typically 1:2 - 1:3 height:width\n- Motion filtering: Eliminate static detections (likely not players)\n- Confidence smoothing: Average confidence across tracking history\n- Non-maximum suppression tuning: Reduce duplicate detections\n\n### 4. Active Learning Pipeline\n- Flag low-confidence detections for manual review\n- Use reviewed detections as training data\n- Iteratively improve model with user corrections\n- Track model version and performance metrics\n\n## Implementation Details\n\n### Files to Create\n- backend/scripts/train_yolo.py - Fine-tuning script\n- backend/app/ml/temporal_smoother.py - Kalman filter implementation\n- backend/app/ml/detection_filters.py - Post-processing filters\n- backend/app/ml/active_learning.py - Active learning pipeline\n\n### Files to Modify\n- backend/app/ml/yolo_detector.py - Load fine-tuned model\n- backend/app/ml/byte_tracker.py - Integrate temporal smoothing\n- backend/app/services/detection_pipeline.py - Apply post-processing filters\n\n### Configuration\n```python\n# config.py additions\nyolo_model_custom: str = \"models/yolo_basketball_finetuned.pt\"\nuse_temporal_smoothing: bool = True\nmin_detection_size: int = 20  # pixels\nmax_detection_size: int = 500\nmin_aspect_ratio: float = 0.3\nmax_aspect_ratio: float = 0.7\n```","acceptance_criteria":"## Automated Tests\n- Unit test: Size filter removes too-small and too-large detections\n- Unit test: Aspect ratio filter rejects non-player shapes\n- Unit test: Temporal smoother reduces bbox jitter\n- Integration test: Fine-tuned model improves accuracy on test set\n\n## Manual Testing\n- Run detection on validation videos with ground truth\n- Measure precision/recall before and after improvements:\n  * Target: \u003e90% precision, \u003e85% recall\n- Count false positives (audience, referees detected as players)\n- Verify tracking stability (fewer ID switches)\n- Check processing time impact (should be minimal)\n\n## Performance Goals\n- Reduce false positive rate by 50% compared to baseline\n- Improve tracking stability (reduce ID switches by 30%)\n- Maintain or improve inference speed\n- F1 score \u003e0.87 on basketball validation set","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T08:04:37.952184-06:00","updated_at":"2025-12-19T08:04:37.952184-06:00"}
{"id":"bbva-6x8","title":"Implement detection API endpoints","description":"REST API endpoints for: triggering video processing, checking processing status, retrieving detection results by video/frame. Includes WebSocket endpoint for real-time progress updates (optional).","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T04:00:52.485444-06:00","updated_at":"2025-12-18T13:29:54.943203-06:00","closed_at":"2025-12-18T13:29:54.943203-06:00"}
{"id":"bbva-788","title":"Implement Players CRUD API endpoints","description":"Manage player database across games","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.004865-06:00","updated_at":"2025-12-07T08:50:31.535588-06:00","closed_at":"2025-12-07T08:50:31.535588-06:00","dependencies":[{"issue_id":"bbva-788","depends_on_id":"bbva-isa","type":"blocks","created_at":"2025-12-06T19:27:42.549994-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-78a","title":"Initialize Python backend with FastAPI and Poetry","description":"Set up backend directory structure, pyproject.toml, FastAPI app skeleton","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:25:42.896185-06:00","updated_at":"2025-12-06T20:14:56.686164-06:00","closed_at":"2025-12-06T20:14:56.686164-06:00"}
{"id":"bbva-7d0","title":"Epic: Phase 4 - Search \u0026 Export","description":"Advanced search, filtering, clip generation, and highlight reels","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-06T19:25:18.04543-06:00","updated_at":"2025-12-06T19:25:18.04543-06:00"}
{"id":"bbva-7k4","title":"Write backend unit tests","description":"pytest tests for services and API endpoints","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.637763-06:00","updated_at":"2025-12-09T07:19:48.559416-06:00","closed_at":"2025-12-09T07:19:48.559416-06:00","dependencies":[{"issue_id":"bbva-7k4","depends_on_id":"bbva-56k","type":"blocks","created_at":"2025-12-06T19:27:00.430986-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-8mv","title":"Implement video upload and storage service","description":"Handle multipart uploads, organize filesystem storage, extract metadata with FFmpeg","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.73131-06:00","updated_at":"2025-12-07T16:57:19.106038-06:00","closed_at":"2025-12-07T16:57:19.106038-06:00","dependencies":[{"issue_id":"bbva-8mv","depends_on_id":"bbva-fft","type":"blocks","created_at":"2025-12-06T19:26:38.66289-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-8xn","title":"Initialize React frontend with Vite and TypeScript","description":"Set up frontend directory structure, package.json, Vite config, basic React app","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:25:42.943258-06:00","updated_at":"2025-12-06T20:21:54.745941-06:00","closed_at":"2025-12-06T20:21:54.745941-06:00"}
{"id":"bbva-a53","title":"Implement player-detection matching service","description":"Link detected players to known roster by matching jersey numbers from OCR with player records. This service enables automatic player identification in videos.","design":"## Current State\n- PlayerDetection table stores bounding boxes with tracking_id but player_id is NULL\n- Player and GameRoster tables track which players are in each game\n- Jersey OCR service will extract numbers from detections\n- No automatic linking between detections and players yet\n\n## Approach\nService to match detections to roster using jersey numbers:\n1. Query jersey OCR results for a video/game\n2. Group by tracking_id, aggregate confidence scores\n3. For each tracking_id, find best jersey number match\n4. Look up player_id from GameRoster where jersey matches\n5. Update PlayerDetection records to set player_id\n6. Handle ambiguity (multiple players same number, low confidence)\n\n## Integration Points\n- Input: Video/Game ID, JerseyNumber records, GameRoster\n- Process: Match jersey numbers to roster, resolve conflicts\n- Output: Update PlayerDetection.player_id\n- Pattern: Similar to detection_pipeline.py - service class with database updates\n\n## Matching Logic\n- Use majority vote across frames for same tracking_id\n- Require minimum confidence threshold (e.g., 0.6)\n- Handle edge cases: no OCR results, multiple matches, no roster match\n- Log ambiguous cases for manual review\n\n## Files to Create\n- backend/app/services/player_matcher.py - Matching service\n- backend/app/api/player_matching.py - API endpoints\n- API endpoint: POST /videos/{video_id}/match-players","acceptance_criteria":"## Automated Tests\n- Unit test: Match single tracking_id to correct player via jersey number\n- Unit test: Majority vote selects correct number when OCR varies across frames\n- Unit test: Handle no OCR results gracefully (leave player_id NULL)\n- Unit test: Handle multiple players with same jersey (flag for manual review)\n- Unit test: Only match players in game roster (ignore jersey numbers not in roster)\n\n## Manual Testing\n- Run detection + OCR on video with known roster\n- Trigger player matching\n- Verify PlayerDetection.player_id correctly set for identified players\n- Check logs for ambiguous cases requiring manual review","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-19T08:02:23.402544-06:00","updated_at":"2025-12-19T08:02:23.402544-06:00","dependencies":[{"issue_id":"bbva-a53","depends_on_id":"bbva-0bw","type":"blocks","created_at":"2025-12-19T08:05:48.737209-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-alb","title":"Set up git repository with .gitignore","description":"Initialize git repo, create .gitignore for Python, Node, videos, and IDE files","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:25:42.991042-06:00","updated_at":"2025-12-06T20:11:59.932798-06:00","closed_at":"2025-12-06T20:11:59.932798-06:00"}
{"id":"bbva-axr","title":"Create OCR aggregation service","description":"Aggregate OCR results per tracking_id","design":"Create backend/app/services/jersey_aggregator.py. Aggregate multiple OCR reads per tracking_id. Use voting/confidence weighting. Detect conflicting reads (swap indicator).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T00:04:36.476361-06:00","updated_at":"2025-12-20T12:19:34.203788-06:00","closed_at":"2025-12-20T12:19:34.203788-06:00"}
{"id":"bbva-azp","title":"Create jersey_number database model","description":"Database model and migration for storing OCR results","design":"Create backend/app/models/jersey_number.py with fields: id, detection_id, tracking_id, video_id, frame_number, raw_ocr_output, parsed_number, confidence, is_valid, created_at. Create alembic migration.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T00:04:14.152093-06:00","updated_at":"2025-12-20T08:57:28.925542-06:00","closed_at":"2025-12-20T08:57:28.925542-06:00"}
{"id":"bbva-bc9","title":"Implement Games CRUD API endpoints","description":"Create, read, update, delete games with FastAPI","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.913668-06:00","updated_at":"2025-12-07T09:03:29.869451-06:00","closed_at":"2025-12-07T09:03:29.869451-06:00","dependencies":[{"issue_id":"bbva-bc9","depends_on_id":"bbva-8mv","type":"blocks","created_at":"2025-12-06T19:26:39.071745-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-bw0","title":"Create user documentation for MVP","description":"Usage guide for video upload, annotation, and playback","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-06T19:26:13.817977-06:00","updated_at":"2025-12-16T14:20:05.788828-06:00","closed_at":"2025-12-16T14:20:05.788828-06:00","dependencies":[{"issue_id":"bbva-bw0","depends_on_id":"bbva-0ty","type":"blocks","created_at":"2025-12-06T19:28:47.92917-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-c6k","title":"Fix tracking stability - ByteTrack integration bugs","design":"Fix critical bugs causing detection/tracking to 'skip in and out inconsistently':\n\n## Root Causes Identified\n\n### Bug 1: Incorrect ByteTrack ID Mapping (CRITICAL)\n- Location: backend/app/ml/byte_tracker.py lines 92-124\n- Problem: _update_tracking_ids() assumes index alignment between original and tracked detections\n- ByteTrack filters and reorders detections, so index-based assignment causes ID swaps\n- Fix: Convert FROM sv.Detections (use returned detections as source of truth)\n\n### Bug 2: Filtering BEFORE Tracking (CRITICAL)\n- Location: backend/app/services/detection_pipeline.py lines 337-355\n- Problem: Court filtering happens before ByteTrack\n- If court detection is inconsistent, players get filtered some frames but not others\n- ByteTrack never sees these detections, can't maintain tracking continuity\n- Fix: Reorder to YOLO â†’ ByteTrack â†’ Court Filter\n\n### Bug 3: Frame Sampling Breaks Tracker Timing (CRITICAL)\n- Location: backend/app/services/detection_pipeline.py line 295\n- Problem: sample_interval=3 means tracker sees ~10 FPS on 30 FPS video\n- Tracker initialized with frame_rate=metadata.fps (30) - motion predictions are 3x off\n- Fix: Pass effective_fps = fps / sample_interval to tracker\n\n### Bug 4: Empty Frames Don't Advance Tracker (HIGH)\n- Location: backend/app/ml/byte_tracker.py lines 49-50\n- Problem: Early return on empty detections skips tracker update\n- ByteTrack needs updates even on empty frames to age lost tracks\n- Fix: Always call update_with_detections()\n\n### Bug 5: Court Detection Fallback Floods Tracker (HIGH)\n- Location: backend/app/ml/court_detector.py lines 75-77\n- Problem: When no court lines detected, returns full-frame mask\n- This passes ALL detections (including audience) to tracking\n- Fix: Cache last good mask, use conservative central fallback\n\n### Bug 6: Parameters Too Strict (MEDIUM)\n- confidence_threshold=0.5 too high (should be 0.35)\n- tracking_iou_threshold=0.8 too strict for fast motion (should be 0.6)\n- tracking_buffer_seconds=1.0 too short (should be 2.0-3.0s)\n\n## Implementation Plan\n1. Fix ByteTrack to convert FROM sv.Detections\n2. Reorder pipeline: track before court filtering\n3. Fix tracker timing for frame sampling\n4. Fix empty frame handling\n5. Add court mask caching with conservative fallback\n6. Tune parameters\n7. Add tracker.reset() at video start\n8. Run tests and validate","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-19T11:18:16.979475-06:00","updated_at":"2025-12-19T11:25:50.552456-06:00","closed_at":"2025-12-19T11:25:50.552456-06:00"}
{"id":"bbva-ck1","title":"Implement detection API endpoints","description":"Create REST API endpoints for triggering video detection, checking job status, and retrieving detection results. Integrates with DetectionPipeline and JobManager services.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T09:06:07.38803-06:00","updated_at":"2025-12-18T09:11:50.620182-06:00","closed_at":"2025-12-18T09:11:50.620182-06:00"}
{"id":"bbva-ckv","title":"Add RF-DETR inference optimization","description":"Call `model.optimize_for_inference()` on the RF-DETR detector to enable JIT compilation. This traces the model with torch.jit.trace for a specific batch size, providing 10-30% inference speedup.\n\nCurrent warning in logs:\n```\nrfdetr.detr - WARNING - Model is not optimized for inference. Latency may be higher than expected.\n```","design":"In `rfdetr_detector.py`:\n1. After loading the model, call `self._model.optimize_for_inference(batch_size=batch_size)`\n2. Pass batch_size from config so JIT compilation matches actual usage\n3. Add config option `rfdetr_jit_compile: bool = True` to enable/disable\n4. Handle the one-time compilation delay gracefully (log message)","acceptance_criteria":"- Warning no longer appears in logs\n- Inference speed improves (measure before/after)\n- Works with configured batch size","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-05T23:28:01.612507-06:00","created_by":"ncerny","updated_at":"2026-01-05T23:28:01.612507-06:00"}
{"id":"bbva-czt","title":"End-to-end CV pipeline testing","description":"Create E2E tests validating the complete computer vision pipeline from video upload through detection, tracking, OCR, and player matching using real test videos.","design":"## Current State\n- All CV components implemented: detection, tracking, court detection\n- OCR and player matching will be added soon\n- No end-to-end integration tests yet\n- Backend has pytest infrastructure\n\n## Test Scenarios\n\n### Scenario 1: Full Pipeline with Known Ground Truth\n1. Upload test video with known roster and visible jerseys\n2. Run detection job and wait for completion\n3. Verify detections created with correct frame numbers\n4. Verify tracking IDs persist across frames\n5. Verify court detection filtered out audience\n6. Run OCR and verify jersey numbers extracted\n7. Run player matching and verify player_id assigned correctly\n8. Compare results to ground truth annotations\n\n### Scenario 2: Error Handling\n- Test video with no detectable court (should fall back to full frame)\n- Test video with no visible players (should complete with 0 detections)\n- Test video with corrupted frames (should skip bad frames)\n- Test cancellation of running job\n\n### Scenario 3: Performance Validation\n- Test large video (\u003e5 minutes) completes within reasonable time\n- Verify API remains responsive during processing\n- Check memory usage stays within limits\n- Verify database size scales linearly with detections\n\n## Test Data Needed\n- Short test videos (10-30 seconds) with ground truth\n- Videos with varying quality, angles, lighting\n- Ground truth JSON files with expected detections/tracking\n\n## Files to Create\n- backend/tests/e2e/test_cv_pipeline.py\n- backend/tests/fixtures/videos/*.mp4 - Test videos\n- backend/tests/fixtures/ground_truth/*.json - Expected results","acceptance_criteria":"## Automated Tests\n- E2E test: Full pipeline produces expected detections (within tolerance)\n- E2E test: Tracking IDs consistent across frames (\u003e90% accuracy)\n- E2E test: Court detection filters \u003e50% of audience detections\n- E2E test: API responsive during long-running detection job\n- E2E test: Job cancellation works correctly\n\n## Manual Testing\n- Run pytest backend/tests/e2e/test_cv_pipeline.py\n- Verify all E2E tests pass\n- Check test execution time \u003c5 minutes for full suite\n- Inspect visual output (optional: save annotated frames for review)","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T08:03:46.882113-06:00","updated_at":"2025-12-19T08:03:46.882113-06:00","dependencies":[{"issue_id":"bbva-czt","depends_on_id":"bbva-eur","type":"blocks","created_at":"2025-12-19T08:06:12.398632-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-d3c","title":"Write ML pipeline unit tests","description":"Create comprehensive unit tests for ML components: YOLO detector, ByteTrack tracker, court detector, and jersey OCR. Ensure each component works correctly in isolation.","design":"## Current State\n- ML components implemented: YOLODetector, PlayerTracker, CourtDetector\n- No unit tests for ML components yet\n- Jersey OCR will be implemented soon\n- Backend uses pytest for testing (see backend/tests/)\n\n## Test Coverage Needed\n\n### YOLODetector (backend/app/ml/yolo_detector.py)\n- Test model loading (mock model to avoid downloading)\n- Test detect_batch with sample frames\n- Test confidence filtering\n- Test bbox coordinate extraction\n- Test person/ball classification\n\n### PlayerTracker (backend/app/ml/byte_tracker.py)\n- Test tracking ID assignment across frames\n- Test track persistence when detection temporarily lost\n- Test IOU matching threshold\n- Test track termination after buffer expires\n- Test handling empty detection lists\n\n### CourtDetector (backend/app/ml/court_detector.py)\n- Test court mask generation from sample frames\n- Test line detection (horizontal/vertical classification)\n- Test boundary extraction edge cases (no lines, too few lines)\n- Test is_point_in_court and is_bbox_in_court\n- Test overlap threshold calculations\n\n### Jersey OCR (backend/app/ml/jersey_ocr.py - future)\n- Test OCR on sample cropped jersey images\n- Test preprocessing pipeline\n- Test number validation (0-99 range)\n- Test confidence scoring\n\n## Files to Create\n- backend/tests/ml/test_yolo_detector.py\n- backend/tests/ml/test_byte_tracker.py\n- backend/tests/ml/test_court_detector.py\n- backend/tests/ml/test_jersey_ocr.py (after OCR implemented)\n- backend/tests/fixtures/sample_frames/ - Test images","acceptance_criteria":"## Automated Tests\nAll tests must pass:\n- pytest backend/tests/ml/ achieves \u003e80% code coverage\n- YOLO tests: 5+ test cases covering main functionality\n- ByteTrack tests: 5+ test cases for tracking logic\n- CourtDetector tests: 6+ test cases for boundary detection\n- Jersey OCR tests: 4+ test cases (after OCR implemented)\n- Tests run without downloading models (use mocks/fixtures)\n\n## Manual Testing\n- Run full test suite: pytest backend/tests/ml/\n- Verify all tests pass\n- Check coverage report shows \u003e80% coverage for ML modules","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T08:03:28.919246-06:00","updated_at":"2025-12-19T08:03:28.919246-06:00"}
{"id":"bbva-d4b","title":"Implement Game List page","description":"List all games, create new games, navigation to detail","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.504034-06:00","updated_at":"2025-12-08T08:56:16.017322-06:00","closed_at":"2025-12-08T08:56:16.017322-06:00","dependencies":[{"issue_id":"bbva-d4b","depends_on_id":"bbva-t11","type":"blocks","created_at":"2025-12-06T19:26:59.877693-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-d4b","depends_on_id":"bbva-bc9","type":"blocks","created_at":"2025-12-06T19:26:59.923579-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-da6","title":"Implement background processing job system","description":"Async background job system for running CV processing on videos. Uses asyncio with task queue pattern. Tracks job status (pending/processing/completed/failed), supports progress tracking, and handles errors gracefully.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-18T04:00:49.276739-06:00","updated_at":"2025-12-18T04:06:36.42553-06:00","closed_at":"2025-12-18T04:06:36.42553-06:00"}
{"id":"bbva-dhn","title":"Create legibility filter module","description":"Implement heuristic legibility checks for jersey crop quality","design":"Create backend/app/ml/legibility_filter.py with checks for: min bbox size, blur detection (Laplacian), confidence threshold, aspect ratio. Return legibility score 0-1. Include unit tests.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T00:03:43.762295-06:00","updated_at":"2025-12-20T00:08:47.429821-06:00","closed_at":"2025-12-20T00:08:47.429821-06:00"}
{"id":"bbva-ecy","title":"Integrate Mantine UI library for theming and components","description":"Replace basic Tailwind styling with Mantine v7 for better theming, consistent UI components, improved user experience, and better performance. Chosen over Ionic (incompatible with React Router v7) and MUI (heavier, Material Design constraints).","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-09T09:48:14.960669-06:00","updated_at":"2025-12-09T13:51:38.859481-06:00","closed_at":"2025-12-09T13:51:38.859481-06:00"}
{"id":"bbva-eur","title":"Implement detection review UI","description":"Create UI for manually reviewing, correcting, and verifying player detections. Users can fix incorrect OCR results, resolve ambiguous matches, and manually assign player_id to detections.","design":"## Current State\n- DetectionOverlay.tsx shows bounding boxes on video (bbva-unh)\n- No UI for reviewing or correcting detection results\n- Player matching may have ambiguous cases requiring manual input\n- Detection pipeline complete (bbva-ntd, bbva-43e, bbva-ck1, bbva-l8b)\n\n## Approach\nCreate review interface with video playback + detection editing:\n1. Show video with detection overlay (reuse DetectionOverlay)\n2. List detected tracking IDs with OCR results and matched players\n3. Allow user to click detection to view details\n4. Provide form to edit jersey number, assign player, adjust confidence\n5. Show unmatched detections (player_id NULL) separately\n6. Filter by confidence, frame range, player\n\n## UI Components Needed\n- DetectionReviewPage.tsx - Main page component\n- DetectionList.tsx - List of detections grouped by tracking_id\n- DetectionEditForm.tsx - Form to edit detection metadata\n- Use existing patterns from GameDetail.tsx for layout\n- Use existing form patterns from player/game CRUD\n\n## API Integration\n- GET /videos/{video_id}/detections (existing - detection.ts)\n- PATCH /detections/{detection_id} (new - update player_id, jersey_number)\n- GET /videos/{video_id}/unmatched-detections (new - filter NULL player_id)\n\n## Files to Create\n- frontend/src/pages/DetectionReview.tsx\n- frontend/src/components/DetectionList.tsx\n- frontend/src/components/DetectionEditForm.tsx\n- backend/app/api/detection.py - Add PATCH endpoint for editing","acceptance_criteria":"## Automated Tests\n- Component test: DetectionList renders grouped detections correctly\n- Component test: DetectionEditForm submits updates to API\n- Component test: Clicking detection in list highlights it in overlay\n- API test: PATCH /detections/{id} updates player_id correctly\n\n## Manual Testing\n- Open detection review page for video with detections\n- Verify all tracking IDs listed with OCR results\n- Click unmatched detection, assign player manually\n- Verify detection overlay highlights selected detection\n- Submit edit and confirm database updated","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-19T08:02:52.43361-06:00","updated_at":"2025-12-19T08:02:52.43361-06:00","dependencies":[{"issue_id":"bbva-eur","depends_on_id":"bbva-a53","type":"blocks","created_at":"2025-12-19T08:06:07.077484-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-ey3","title":"Epic: Project Setup \u0026 Infrastructure","description":"Initialize project structure, documentation, and development environment","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:25:02.375167-06:00","updated_at":"2025-12-16T14:17:44.491099-06:00","closed_at":"2025-12-16T14:17:44.491099-06:00","dependencies":[{"issue_id":"bbva-ey3","depends_on_id":"bbva-78a","type":"parent-child","created_at":"2025-12-06T19:26:38.244425-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ey3","depends_on_id":"bbva-8xn","type":"parent-child","created_at":"2025-12-06T19:26:38.292269-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ey3","depends_on_id":"bbva-alb","type":"parent-child","created_at":"2025-12-06T19:26:38.339675-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ey3","depends_on_id":"bbva-fkw","type":"parent-child","created_at":"2025-12-06T19:26:38.385719-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-f83","title":"Implement player/roster management UI","description":"Player database UI, game roster assignment, jersey number management","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.412879-06:00","updated_at":"2025-12-08T13:59:22.527065-06:00","closed_at":"2025-12-08T13:59:22.527065-06:00","dependencies":[{"issue_id":"bbva-f83","depends_on_id":"bbva-rx5","type":"blocks","created_at":"2025-12-06T19:26:59.647845-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-fft","title":"Set up Alembic and create initial migration","description":"Initialize Alembic, create first migration script for all tables","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.638473-06:00","updated_at":"2025-12-06T23:27:05.464593-06:00","closed_at":"2025-12-06T23:27:05.464593-06:00","dependencies":[{"issue_id":"bbva-fft","depends_on_id":"bbva-isa","type":"blocks","created_at":"2025-12-06T19:26:38.571775-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-fkw","title":"Create project README and setup documentation","description":"Document project setup, installation instructions, and development workflow","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:25:43.036746-06:00","updated_at":"2025-12-06T20:24:06.804233-06:00","closed_at":"2025-12-06T20:24:06.804233-06:00"}
{"id":"bbva-g2m","title":"Epic: Phase 2 - Computer Vision Integration","description":"Semi-automatic player detection, tracking, and jersey number recognition","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:25:17.949193-06:00","updated_at":"2025-12-06T19:25:17.949193-06:00"}
{"id":"bbva-gl2","title":"Cloud GPU processing for SAM3 video detection","description":"SAM3 tracking quality is excellent but requires ~60GB+ memory for long videos, making local processing on 64GB MacBook impractical (causes heavy paging, ~25 days for 1-hour video).\n\nCloud GPU (A100 80GB) would:\n- Fit entire video in VRAM (no paging)\n- Process ~10x faster (CUDA vs MPS)\n- Cost ~$1-2/hour, so $2-4 per hour-long video\n\n## Requirements\n- Worker process must be separable from API (blocked by bbva-61o)\n- Support for remote job execution\n- Progress reporting back to API\n- Result storage and retrieval\n\n## Cloud Options\n- Lambda Labs: A100 80GB ~$1.10/hr\n- RunPod: A100 80GB ~$1.19/hr\n- Vast.ai: Variable pricing\n\n## Implementation Approach\n1. Containerize worker with SAM3 dependencies\n2. Job queue picks up pending detection jobs\n3. Worker pulls video, processes, uploads results\n4. API polls for completion","design":"## Design Document\n\nSee `docs/plans/2026-01-22-cloud-gpu-processing-design.md` for full design.\n\n## Summary\n\n**Architecture:** R2 file-based queue\n- Local API uploads video + job manifest to R2\n- Cloud worker polls R2, processes, uploads results\n- Local CLI imports results into SQLite\n\n**Workflow:**\n1. `python -m worker.cli submit --video-id 1` (uploads to R2)\n2. Manually start cloud GPU, run `docker run ... basketball-analyzer-worker`\n3. `python -m worker.cli import --all` (pulls results into local DB)\n\n**Key Components:**\n- `worker/cloud_storage.py` - R2 operations\n- `worker/cloud_worker.py` - Cloud worker (polls R2)\n- `worker/cli.py` - Submit/import/status commands\n- `Dockerfile.worker` - Container with SAM3 model baked in\n\n**Model Packaging:**\n- SAM3 model baked into Docker image (~8-10GB)\n- Avoids HuggingFace auth on cloud\n- Local-first loading: checks /models/sam3 before downloading","acceptance_criteria":"- [ ] R2 bucket created with correct structure (videos/, jobs/, results/, status/)\n- [ ] CLI can submit jobs: uploads video + manifest to R2\n- [ ] Cloud worker polls R2 and processes pending jobs\n- [ ] Cloud worker uploads results to R2 on completion\n- [ ] CLI can import completed results into local SQLite\n- [ ] CLI can show status of all cloud jobs\n- [ ] Dockerfile builds with SAM3 model baked in\n- [ ] Worker loads model from local path (no HF download)\n- [ ] End-to-end test: submit â†’ cloud process â†’ import","notes":"Implementation complete. All 8 tasks done:\\n- R2 cloud storage module with boto3\\n- CLI for job management (submit, status, import)\\n- Cloud worker that polls R2\\n- Dockerfile with SAM3 model baked in\\n- Local-first model loading for Docker\\n\\nReady for manual E2E testing once R2 is configured.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-21T19:13:19.19671-06:00","updated_at":"2026-01-22T10:29:37.68458-06:00","dependencies":[{"issue_id":"bbva-gl2","depends_on_id":"bbva-61o","type":"blocks","created_at":"2026-01-21T19:13:19.20166-06:00","created_by":"daemon"}]}
{"id":"bbva-icp","title":"Enable GPU/NPU acceleration for ML inference","description":"Optimize ML pipeline performance by enabling GPU acceleration (CUDA for NVIDIA, MPS for Apple Silicon) and exploring Apple Neural Engine for YOLOv8 inference. Improve processing speed for detection and tracking.","design":"## Current State\n- Device auto-detection implemented: CUDA â†’ MPS â†’ CPU fallback (detection_pipeline.py:87-101)\n- YOLOv8 supports GPU via PyTorch device parameter\n- Currently using asyncio.to_thread() for CPU-intensive operations\n- No explicit GPU optimization or benchmarking yet\n\n## Approach\n\n### Phase 1: GPU Acceleration\n1. Verify CUDA/MPS support in current environment\n2. Enable GPU for YOLO inference (already supported via device parameter)\n3. Move ByteTrack to GPU if beneficial (requires profiling)\n4. Keep court detection on CPU (OpenCV operations not GPU-intensive)\n\n### Phase 2: Apple Neural Engine (ANE)\n1. Convert YOLOv8 model to CoreML format for ANE support\n2. Use coremltools to compile model with ANE optimization\n3. Fall back to GPU/CPU if ANE unavailable\n4. Benchmark ANE vs MPS vs CPU performance\n\n### Phase 3: Batch Optimization\n1. Increase batch_size when GPU available (more VRAM = larger batches)\n2. Experiment with batch sizes: 8 (CPU), 16 (MPS), 32 (CUDA)\n3. Profile memory usage vs speed tradeoffs\n4. Add auto-tuning based on available VRAM\n\n## Implementation Details\n\n### Files to Modify\n- backend/app/ml/yolo_detector.py - Add CoreML support, batch size tuning\n- backend/app/services/detection_pipeline.py - Adjust batch_size based on device\n- backend/app/config.py - Add GPU-specific config (batch_size_gpu, use_ane)\n- backend/requirements.txt - Add coremltools for ANE support\n\n### Configuration\n```python\n# config.py additions\nyolo_batch_size_cpu: int = 8\nyolo_batch_size_gpu: int = 32\nenable_apple_neural_engine: bool = True\n```\n\n### Benchmarking\n- Add performance logging: frames/second, inference time per batch\n- Create benchmark script to compare CPU/MPS/CUDA/ANE\n- Document optimal settings for each platform","acceptance_criteria":"## Automated Tests\n- Unit test: Device detection correctly identifies available hardware\n- Unit test: Batch size adjusts based on device type\n- Unit test: CoreML model produces same results as PyTorch model (within tolerance)\n- Integration test: GPU pipeline produces correct detections\n\n## Manual Testing\n- Run detection on same video with CPU, MPS, and CUDA (if available)\n- Verify GPU utilization increases during inference (nvidia-smi or Activity Monitor)\n- Measure and compare processing speed: frames/second for each device\n- Confirm ANE model works on Apple Silicon (if available)\n- Verify no accuracy regression with GPU vs CPU\n\n## Performance Goals\n- 2-3x speedup on MPS (Apple Silicon) vs CPU\n- 4-6x speedup on CUDA (NVIDIA) vs CPU\n- ANE comparable or better than MPS (if supported)","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T08:04:12.030417-06:00","updated_at":"2025-12-19T08:23:00.160986-06:00","closed_at":"2025-12-19T08:23:00.160986-06:00"}
{"id":"bbva-ifo","title":"Epic: Phase 1 - MVP Foundation","description":"Video management, unified game player, manual annotation, and player roster management","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:25:17.902854-06:00","updated_at":"2025-12-15T16:16:51.401805-06:00","closed_at":"2025-12-15T16:16:51.401805-06:00","dependencies":[{"issue_id":"bbva-ifo","depends_on_id":"bbva-isa","type":"parent-child","created_at":"2025-12-06T19:26:38.432175-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-fft","type":"parent-child","created_at":"2025-12-06T19:26:38.479521-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-45w","type":"parent-child","created_at":"2025-12-06T19:26:38.524303-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-bc9","type":"parent-child","created_at":"2025-12-06T19:26:38.842893-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-sl9","type":"parent-child","created_at":"2025-12-06T19:26:38.889007-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-788","type":"parent-child","created_at":"2025-12-06T19:26:38.932171-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-x6c","type":"parent-child","created_at":"2025-12-06T19:26:38.979429-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-56k","type":"parent-child","created_at":"2025-12-06T19:26:39.025421-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-rx5","type":"parent-child","created_at":"2025-12-06T19:26:59.001512-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-wi7","type":"parent-child","created_at":"2025-12-06T19:26:59.049363-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-6c1","type":"parent-child","created_at":"2025-12-06T19:26:59.09562-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-jb6","type":"parent-child","created_at":"2025-12-06T19:26:59.145184-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-4xa","type":"parent-child","created_at":"2025-12-06T19:26:59.191514-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-4m7","type":"parent-child","created_at":"2025-12-06T19:26:59.237973-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-f83","type":"parent-child","created_at":"2025-12-06T19:26:59.284056-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-t11","type":"parent-child","created_at":"2025-12-06T19:26:59.330705-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-d4b","type":"parent-child","created_at":"2025-12-06T19:26:59.739342-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-0t9","type":"parent-child","created_at":"2025-12-06T19:26:59.787558-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-0ty","type":"parent-child","created_at":"2025-12-06T19:26:59.832469-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-7k4","type":"parent-child","created_at":"2025-12-06T19:27:00.196998-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-qqg","type":"parent-child","created_at":"2025-12-06T19:27:00.248357-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-1fw","type":"parent-child","created_at":"2025-12-06T19:27:00.293435-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-s5h","type":"parent-child","created_at":"2025-12-06T19:27:00.339101-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-bw0","type":"parent-child","created_at":"2025-12-06T19:27:00.385593-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-8mv","type":"parent-child","created_at":"2025-12-06T19:27:17.469186-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-03i","type":"parent-child","created_at":"2025-12-06T19:27:17.517975-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-y0h","type":"parent-child","created_at":"2025-12-06T19:27:17.565827-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-ryz","type":"parent-child","created_at":"2025-12-06T19:27:17.612691-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-ifo","depends_on_id":"bbva-ey3","type":"related","created_at":"2025-12-06T19:27:17.707667-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-isa","title":"Design and implement SQLAlchemy models","description":"Create models for games, videos, players, game_rosters, annotations, annotation_videos, plays, player_detections","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.590347-06:00","updated_at":"2025-12-06T22:58:42.941467-06:00","closed_at":"2025-12-06T22:58:42.941467-06:00","dependencies":[{"issue_id":"bbva-isa","depends_on_id":"bbva-78a","type":"blocks","created_at":"2025-12-06T19:27:17.660516-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-jb6","title":"Implement timeline visualization component","description":"Timeline bar with video segments, gaps, overlaps, color coding","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:13.275584-06:00","updated_at":"2025-12-09T07:55:18.00481-06:00","closed_at":"2025-12-09T07:55:18.00481-06:00","dependencies":[{"issue_id":"bbva-jb6","depends_on_id":"bbva-rx5","type":"blocks","created_at":"2025-12-06T19:26:59.512294-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-l8b","title":"Implement court detection for spatial filtering","description":"Add automatic court line detection using Hough transform to identify court boundaries and filter out audience member detections","design":"Use OpenCV Hough line detection to find court lines in each frame. Build polygon mask from detected lines. Filter YOLO detections to only include those within court boundaries. Handle dynamic camera movements (pan/zoom). Cache court detection results per video to avoid reprocessing every frame.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T23:58:40.439798-06:00","updated_at":"2025-12-19T07:08:14.396952-06:00","closed_at":"2025-12-19T07:08:14.396952-06:00"}
{"id":"bbva-ntd","title":"Implement player detection pipeline service","description":"End-to-end pipeline that extracts frames, runs YOLO detection, and stores results. Integrates frame extractor + YOLODetector + database storage. Includes progress callbacks for UI feedback.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-18T04:00:50.960964-06:00","updated_at":"2025-12-18T07:43:30.165245-06:00","closed_at":"2025-12-18T07:43:30.165245-06:00"}
{"id":"bbva-o1u","title":"Investigate SAM2Long for bounded memory tracking","description":"Research found SAM2Long as a promising solution for our memory issues.\n\n## SAM2Long Key Features\n- **Bounded memory**: Uses constrained tree memory structure (P=3 pathways)\n- **Only +4% memory overhead** over vanilla SAM2\n- **Only +8% compute / -14% FPS** tradeoff\n- Works with all SAM2 checkpoints (tiny, small, base+, large)\n- Training-free, drop-in enhancement\n- Accepted at ICCV 2025, code fully released\n\n## Why This Matters\nCurrent SAM3 uses ~60GB+ for 8000 frames due to unbounded memory growth.\nSAM2Long could bound this to ~6-10GB with sam2-hiera-tiny.\n\n## SAM2 Model Variants Available\n| Model | Parameters | VRAM |\n|-------|-----------|------|\n| sam2-hiera-tiny | 38.9M | 6-8 GB |\n| sam2-hiera-small | 46M | 8-10 GB |\n| sam2-hiera-base-plus | 80.8M | 10-12 GB |\n| sam2-hiera-large | 224.4M | 16+ GB |\n\n## Questions to Answer\n1. Does SAM2Long work with SAM3's text prompting?\n2. Can we integrate SAM2Long's tree memory into our pipeline?\n3. What's tracking quality like compared to our current SAM3 approach?\n4. Is sam2-hiera-tiny sufficient quality for basketball tracking?\n\n## Resources\n- GitHub: https://github.com/Mark12Ding/SAM2Long\n- Paper: https://arxiv.org/html/2410.16268v3\n- Project: https://mark12ding.github.io/project/SAM2Long/","notes":"IMPORTANT CONTEXT: We tried vanilla SAM2 and experienced severe ID drift issues. SAM3 is the only approach that achieved stable tracking.\n\nSAM2Long might address this - it's specifically designed to prevent \"error accumulation\" in long videos using quality-based frame selection and multiple memory pathways. However, this is speculative and would need testing.\n\nIf SAM2Long doesn't fix the ID drift, cloud GPU for SAM3 (bbva-gl2) is the only viable path.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2026-01-21T19:13:19.868398-06:00","updated_at":"2026-01-21T19:54:06.96592-06:00"}
{"id":"bbva-qbe","title":"Epic: Phase 5 - Advanced Multi-Angle Features","description":"Enhanced multi-angle viewing and automatic video synchronization","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-06T19:25:18.094594-06:00","updated_at":"2025-12-06T19:25:18.094594-06:00"}
{"id":"bbva-qqg","title":"Write frontend component tests","description":"Vitest tests for critical components","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-06T19:26:13.681781-06:00","updated_at":"2025-12-15T16:24:26.736085-06:00","closed_at":"2025-12-15T16:24:26.736085-06:00","dependencies":[{"issue_id":"bbva-qqg","depends_on_id":"bbva-0ty","type":"blocks","created_at":"2025-12-06T19:27:00.477064-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-qvv","title":"Migrate to Convex.dev cloud database","description":"**Future State - Not Ready for Implementation**\n\nEvaluate migrating from local SQLite to Convex.dev as a unified cloud database. This would provide:\n- Real-time progress updates (Convex is reactive)\n- Single source of truth for jobs and detections\n- Simpler architecture (no file sync/import)\n- Path to fully cloud-native app\n\n## Blockers Before Implementation\n- Need to validate Convex free tier limits vs detection volume (~50MB/game)\n- Requires schema rewrite from SQLAlchemy to Convex\n- Need to evaluate cost at scale (20+ games/season)\n- Depends on cloud GPU processing being stable first (bbva-gl2)\n\n## Open Questions\n- Can Convex handle 1M+ rows per game efficiently?\n- What's the migration path for existing data?\n- How does Convex pricing scale with detection volume?","status":"open","priority":4,"issue_type":"feature","created_at":"2026-01-22T08:05:00.247471-06:00","updated_at":"2026-01-22T08:05:00.247471-06:00","labels":["future-state","needs-research"],"dependencies":[{"issue_id":"bbva-qvv","depends_on_id":"bbva-gl2","type":"blocks","created_at":"2026-01-22T08:05:05.925093-06:00","created_by":"daemon"}]}
{"id":"bbva-rx5","title":"Implement unified game timeline state management","description":"Zustand store for game time, video sequencing, playback state","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.140203-06:00","updated_at":"2025-12-06T22:48:49.986173-06:00","closed_at":"2025-12-06T22:48:49.986173-06:00","dependencies":[{"issue_id":"bbva-rx5","depends_on_id":"bbva-8xn","type":"blocks","created_at":"2025-12-06T19:26:59.376239-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-ryz","title":"Implement video thumbnail generation service","description":"Generate thumbnail images for video preview","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:26:12.867378-06:00","updated_at":"2025-12-08T12:09:33.234945-06:00","closed_at":"2025-12-08T12:09:33.234945-06:00","dependencies":[{"issue_id":"bbva-ryz","depends_on_id":"bbva-fft","type":"blocks","created_at":"2025-12-06T19:26:38.798495-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-s5h","title":"Performance optimization for large videos","description":"Optimize video loading, buffering, and playback performance","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-06T19:26:13.772447-06:00","updated_at":"2025-12-15T16:26:52.808483-06:00","closed_at":"2025-12-15T16:26:52.808483-06:00","dependencies":[{"issue_id":"bbva-s5h","depends_on_id":"bbva-1fw","type":"blocks","created_at":"2025-12-06T19:27:00.56862-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-sak","title":"Epic: Phase 3 - Play Recognition","description":"Automatic play detection and categorization with user verification","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-06T19:25:17.999065-06:00","updated_at":"2025-12-06T19:25:17.999065-06:00"}
{"id":"bbva-sbi","title":"Add jersey number API endpoints","description":"API endpoints for jersey number OCR results","design":"Add to detection.py: GET /api/videos/{id}/jersey-numbers, GET /api/videos/{id}/tracks/{track_id}/jersey-number, POST /api/videos/{id}/run-ocr. Return aggregated numbers per track.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T00:04:40.180153-06:00","updated_at":"2025-12-20T12:19:34.44573-06:00","closed_at":"2025-12-20T12:19:34.44573-06:00"}
{"id":"bbva-sl9","title":"Implement Videos CRUD API endpoints","description":"Upload, list, retrieve video with timeline data","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.960043-06:00","updated_at":"2025-12-07T09:33:17.196966-06:00","closed_at":"2025-12-07T09:33:17.196966-06:00","dependencies":[{"issue_id":"bbva-sl9","depends_on_id":"bbva-8mv","type":"blocks","created_at":"2025-12-06T19:26:39.116424-06:00","created_by":"daemon","metadata":"{}"},{"issue_id":"bbva-sl9","depends_on_id":"bbva-y0h","type":"blocks","created_at":"2025-12-06T19:26:39.161453-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-t11","title":"Implement API client service","description":"Frontend service for calling backend APIs with proper error handling","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.458616-06:00","updated_at":"2025-12-06T22:58:42.98415-06:00","closed_at":"2025-12-06T22:58:42.98415-06:00","dependencies":[{"issue_id":"bbva-t11","depends_on_id":"bbva-8xn","type":"blocks","created_at":"2025-12-06T19:26:59.693816-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-unh","title":"Implement detection overlay component","description":"Frontend component to display ML detection bounding boxes on video player","design":"React component that overlays bounding boxes on MultiVideoPlayer. Fetch detections from API, filter by current frame, render SVG overlay with boxes. Support showing/hiding overlay, color-coding by tracking_id, display confidence scores.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-18T18:17:43.001556-06:00","updated_at":"2025-12-18T18:22:07.262773-06:00","closed_at":"2025-12-18T18:22:07.262773-06:00"}
{"id":"bbva-wi7","title":"Implement multi-video playback engine","description":"Handle video transitions, pre-buffering, game time to video time conversion","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.18598-06:00","updated_at":"2025-12-07T22:06:46.405278-06:00","closed_at":"2025-12-07T22:06:46.405278-06:00","dependencies":[{"issue_id":"bbva-wi7","depends_on_id":"bbva-rx5","type":"blocks","created_at":"2025-12-06T19:26:59.421929-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-x6c","title":"Implement Game Rosters API endpoints","description":"Assign players to games, manage jersey numbers","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:13.050121-06:00","updated_at":"2025-12-07T10:09:27.899152-06:00","closed_at":"2025-12-07T10:09:27.899152-06:00","dependencies":[{"issue_id":"bbva-x6c","depends_on_id":"bbva-788","type":"blocks","created_at":"2025-12-06T19:26:39.206552-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-y0h","title":"Implement game timeline sequencing service","description":"Video sequencing algorithm, game_time_offset calculation, overlap/gap detection","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T19:26:12.823451-06:00","updated_at":"2025-12-07T17:01:27.931589-06:00","closed_at":"2025-12-07T17:01:27.931589-06:00","dependencies":[{"issue_id":"bbva-y0h","depends_on_id":"bbva-fft","type":"blocks","created_at":"2025-12-06T19:26:38.753122-06:00","created_by":"daemon","metadata":"{}"}]}
{"id":"bbva-yoy","title":"Epic: Phase 6 - Desktop App","description":"Electron packaging and native desktop distribution","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-06T19:25:18.141114-06:00","updated_at":"2025-12-06T19:25:18.141114-06:00"}
